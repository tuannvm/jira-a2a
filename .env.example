# jira-a2a Configuration File
# Copy this file to .env and modify as needed

#######################
# Server Configuration
#######################
# Port for the main A2A server
SERVER_PORT=8085
# Host to bind the server to (use 0.0.0.0 to listen on all interfaces)
SERVER_HOST=localhost
# Port for the webhook endpoint (used by JiraRetrievalAgent)
WEBHOOK_PORT=8083

#######################
# Agent Configuration
#######################
# Agent name - determines which agent to run
# Options: JiraRetrievalAgent, InformationGatheringAgent, CopilotAgent
AGENT_NAME=InformationGatheringAgent
# Agent version shown in API responses
AGENT_VERSION=1.0.0
# Public URL for the agent (override if behind a proxy/load balancer)
# If not set, defaults to http://{SERVER_HOST}:{SERVER_PORT}
AGENT_URL=http://localhost:8085

#######################
# Jira Configuration
#######################
# Base URL of your Jira instance
JIRA_BASE_URL=https://your-jira-instance.atlassian.net
# Jira username (usually email address)
JIRA_USERNAME=your-jira-username
# Jira API token (create one in your Atlassian account settings)
JIRA_API_TOKEN=your-jira-api-token

#######################
# Authentication
#######################
# Authentication type: "jwt" or "apikey"
AUTH_TYPE=apikey
# API key for authentication (when AUTH_TYPE=apikey)
API_KEY=your-api-key
# JWT secret for authentication (when AUTH_TYPE=jwt)
JWT_SECRET=your-jwt-secret

#######################
# LLM Configuration
#######################
# Enable LLM integration for enhanced analysis
LLM_ENABLED=false
# LLM provider: openai, azure, or anthropic
LLM_PROVIDER=openai
# Model to use (e.g., gpt-4, gpt-3.5-turbo, claude-2)
LLM_MODEL=gpt-4
# API key for the LLM provider (can also be set via OPENAI_API_KEY environment variable)
LLM_API_KEY=your-openai-api-key
# Service URL (required for Azure OpenAI or custom endpoints)
LLM_SERVICE_URL=
# Maximum tokens to generate in responses
LLM_MAX_TOKENS=4000
# Timeout in seconds for LLM requests
LLM_TIMEOUT=30
# Temperature controls randomness (0.0-1.0, lower is more deterministic)
LLM_TEMPERATURE=0.0
